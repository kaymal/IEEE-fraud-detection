{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('dataset/train_mem.pkl')\n",
    "train.set_index('TransactionID', drop=True, inplace=True)\n",
    "\n",
    "test = pd.read_pickle('dataset/test_mem.pkl')\n",
    "test.set_index('TransactionID', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test):\n",
    "    \n",
    "    ## PREPROCESSING\n",
    "    # Preprocessing for unique values in the test set (e-mail columns)\n",
    "    P_email_list = train.P_emaildomain.unique().tolist()\n",
    "    test.P_emaildomain.loc[~test.P_emaildomain.isin(P_email_list)] = np.nan\n",
    "    \n",
    "    R_email_list = train.R_emaildomain.unique().tolist()\n",
    "    test.R_emaildomain.loc[~test.R_emaildomain.isin(R_email_list)] = np.nan\n",
    "    \n",
    "    # Concatenate train and test data\n",
    "    data = pd.concat((train.drop('isFraud', axis=1), test)).copy()\n",
    "\n",
    "    ## FEATURE SELECTION\n",
    "    # List of columns/features to be removed from the adtaaset\n",
    "    remove_list = (#['TransactionAmt'] # new (different) variable created\n",
    "                   ['C2', 'C4', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14'] # correlation\n",
    "                  +['D2', 'D6', 'D7', 'D12'] # correlation\n",
    "                  +['V5', 'V9', 'V11', 'V13', 'V16', 'V17', 'V18', 'V20', 'V21', 'V22', 'V24', \n",
    "                    'V26', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V36', 'V38', 'V39', \n",
    "                    'V40', 'V41', 'V42', 'V43', 'V45', 'V48', 'V49', 'V50', 'V51', 'V52', 'V54', \n",
    "                    'V57', 'V58', 'V59', 'V60', 'V62', 'V63', 'V64', 'V65', 'V67', 'V68', 'V69', \n",
    "                    'V70', 'V71', 'V72', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V84', \n",
    "                    'V85', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V96', 'V97', \n",
    "                    'V100', 'V101', 'V102', 'V103', 'V105', 'V106', 'V110', 'V113', 'V116', 'V119', \n",
    "                    'V125', 'V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V137', 'V140', 'V142', \n",
    "                    'V143', 'V145', 'V147', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', \n",
    "                    'V156', 'V157', 'V158', 'V159', 'V160', 'V162', 'V163', 'V164', 'V165', 'V167', \n",
    "                    'V168', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V185', 'V186', \n",
    "                    'V189', 'V190', 'V191', 'V192', 'V193', 'V195', 'V196', 'V197', 'V198', 'V199', \n",
    "                    'V200', 'V201', 'V202', 'V203', 'V204', 'V206', 'V207', 'V211', 'V212', 'V213', \n",
    "                    'V216', 'V217', 'V218', 'V219', 'V222', 'V225', 'V228', 'V229', 'V230', 'V231', \n",
    "                    'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V239', 'V242', 'V243', 'V244', \n",
    "                    'V245', 'V246', 'V247', 'V248', 'V249', 'V251', 'V252', 'V253', 'V254', 'V255', \n",
    "                    'V256', 'V257', 'V258', 'V259', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', \n",
    "                    'V267', 'V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', \n",
    "                    'V278', 'V279', 'V280', 'V285', 'V287', 'V289', 'V292', 'V293', 'V294', 'V295', \n",
    "                    'V296', 'V297', 'V298', 'V299', 'V301', 'V302', 'V303', 'V304', 'V306', 'V307', \n",
    "                    'V308', 'V309', 'V310', 'V311', 'V312', 'V315', 'V316', 'V317', 'V318', 'V319', \n",
    "                    'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', \n",
    "                    'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339'] # correlation\n",
    "                  ## FOR NOW\n",
    "                  +['DeviceType', 'DeviceInfo'] # Needs feature engineering and missing val analysis\n",
    "                  +['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', \n",
    "                     'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "                     'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', \n",
    "                     'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', \n",
    "                     'id_37', 'id_38'] # Needs feature engineering and missing val analysis\n",
    "                  ) \n",
    "    # Remove unnecessary columns\n",
    "    data.drop(remove_list, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    ## TRANSACTION \n",
    "    # Create a new column with the log of TransactionAmt\n",
    "    data['TransactionAmt_log'] = np.log(data.TransactionAmt)\n",
    "    \n",
    "    # Datetime\n",
    "    import datetime\n",
    "    START_DATE = '2019-03-01'\n",
    "    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    data['TransactionDT_DT'] = data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "    \n",
    "    # Hour column\n",
    "    hours = data['TransactionDT']/3600\n",
    "    data['Transaction_hours'] = np.floor(hours) % 24\n",
    "\n",
    "    ## E-MAIL \n",
    "    # Fill NaNs with \"missing\"\n",
    "    data.P_emaildomain.fillna('missing', inplace=True)\n",
    "    data.P_emaildomain.fillna('missing', inplace=True)\n",
    "\n",
    "    # Create dict for each columns (using train data)\n",
    "    P_email_dict = train.groupby('P_emaildomain').isFraud.mean().to_dict()\n",
    "    R_email_dict = train.groupby('R_emaildomain').isFraud.mean().to_dict()\n",
    "\n",
    "    # Create new numeric columns\n",
    "    data['P_emaildomain_val'] = data.P_emaildomain.map(P_email_dict)\n",
    "    data['R_emaildomain_val'] = data.R_emaildomain.map(R_email_dict)\n",
    "    \n",
    " \n",
    "    ## PREPARE DATA FOR MODEL\n",
    "    # List of columns to be dropped from the dataset for modeling\n",
    "    drop_list = (['TransactionDT'] # new (same) variable created\n",
    "                +['TransactionDT_DT'] # datetime \n",
    "                +['P_emaildomain', 'R_emaildomain'] # new (different) variable created\n",
    "                )\n",
    "    \n",
    "    # Create a list of columns with more than 20% missing \n",
    "    remove_missing_cols = data.isna().mean()[data.isna().mean()>0.5].index.to_list()\n",
    "    \n",
    "    # Drop unnecessary columns \n",
    "    data.drop(drop_list+remove_missing_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Create dummy variables\n",
    "    data = pd.get_dummies(data)\n",
    "    \n",
    "    # Split train and test data\n",
    "    train_data = data[:train.shape[0]]\n",
    "    test_data = data[train.shape[0]:]\n",
    "    \n",
    "    return train_data.join(train.isFraud), test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train, test = feature_engineering(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 110), (506691, 109))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train, 'dataset/train_engineered_new.pkl')\n",
    "pd.to_pickle(test, 'dataset/test_engineered_new.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns/features to be removed from the model\n",
    "remove_list = (#['TransactionAmt'] # new (different) variable created\n",
    "               ['TransactionDT_DT'] # datetime \n",
    "              +['C2', 'C4', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14'] # correlation\n",
    "              +['D2', 'D6', 'D7', 'D12'] # correlation\n",
    "              +['P_emaildomain', 'R_emaildomain'] # new (different) variable created\n",
    "              +['V5', 'V9', 'V11', 'V13', 'V16', 'V17', 'V18', 'V20', 'V21', 'V22', 'V24', \n",
    "                'V26', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V36', 'V38', 'V39', \n",
    "                'V40', 'V41', 'V42', 'V43', 'V45', 'V48', 'V49', 'V50', 'V51', 'V52', 'V54', \n",
    "                'V57', 'V58', 'V59', 'V60', 'V62', 'V63', 'V64', 'V65', 'V67', 'V68', 'V69', \n",
    "                'V70', 'V71', 'V72', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V84', \n",
    "                'V85', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V96', 'V97', \n",
    "                'V100', 'V101', 'V102', 'V103', 'V105', 'V106', 'V110', 'V113', 'V116', 'V119', \n",
    "                'V125', 'V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V137', 'V140', 'V142', \n",
    "                'V143', 'V145', 'V147', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', \n",
    "                'V156', 'V157', 'V158', 'V159', 'V160', 'V162', 'V163', 'V164', 'V165', 'V167', \n",
    "                'V168', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V185', 'V186', \n",
    "                'V189', 'V190', 'V191', 'V192', 'V193', 'V195', 'V196', 'V197', 'V198', 'V199', \n",
    "                'V200', 'V201', 'V202', 'V203', 'V204', 'V206', 'V207', 'V211', 'V212', 'V213', \n",
    "                'V216', 'V217', 'V218', 'V219', 'V222', 'V225', 'V228', 'V229', 'V230', 'V231', \n",
    "                'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V239', 'V242', 'V243', 'V244', \n",
    "                'V245', 'V246', 'V247', 'V248', 'V249', 'V251', 'V252', 'V253', 'V254', 'V255', \n",
    "                'V256', 'V257', 'V258', 'V259', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', \n",
    "                'V267', 'V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', \n",
    "                'V278', 'V279', 'V280', 'V285', 'V287', 'V289', 'V292', 'V293', 'V294', 'V295', \n",
    "                'V296', 'V297', 'V298', 'V299', 'V301', 'V302', 'V303', 'V304', 'V306', 'V307', \n",
    "                'V308', 'V309', 'V310', 'V311', 'V312', 'V315', 'V316', 'V317', 'V318', 'V319', \n",
    "                'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', \n",
    "                'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339'] # correlation\n",
    "              ## FOR NOW\n",
    "              + ['DeviceType', 'DeviceInfo'] # Needs feature engineering and missing val analysis\n",
    "              + ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', \n",
    "                 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "                 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', \n",
    "                 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', \n",
    "                 'id_37', 'id_38'] # Needs feature engineering and missing val analysis\n",
    "              ) \n",
    "\n",
    "# List of columns to be removed from the dataset for good\n",
    "drop_list = (['TransactionDT'] # new (same) variable created\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the log of TransactionAmt\n",
    "train['TransactionAmt_log'] = np.log(train.TransactionAmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "START_DATE = '2019-03-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "train['TransactionDT_DT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hour_feature(df, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    tname : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    hours = df[tname] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours\n",
    "\n",
    "train['Transaction_hours'] = make_hour_feature(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with \"missing\"\n",
    "train.P_emaildomain.fillna('missing', inplace=True)\n",
    "train.R_emaildomain = train.P_emaildomain.fillna('missing')\n",
    "\n",
    "# Create dict for each columns (-> use these dicts in for converting TEST DATA)\n",
    "P_email_dict = train.groupby('P_emaildomain').isFraud.mean().to_dict()\n",
    "R_email_dict = train.groupby('R_emaildomain').isFraud.mean().to_dict()\n",
    "\n",
    "# Create new numeric columns\n",
    "train['P_emaildomain_val'] = train.P_emaildomain.map(P_email_dict)\n",
    "train['R_emaildomain_val'] = train.R_emaildomain.map(R_email_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-96bf6b4da8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Apply the encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             Xi = check_array(Xi, ensure_2d=False, dtype=None,\n\u001b[0;32m---> 67\u001b[0;31m                              force_all_finite=needs_validation)\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mX_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create the encoder.\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "encoder.fit(train)\n",
    "\n",
    "# Apply the encoder.\n",
    "train_encoded = encoder.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 439)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_list + drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train = train.drop(remove_list + drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 144)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(save_train, 'dataset/train_engineered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
