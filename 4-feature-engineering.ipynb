{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('dataset/train_mem.pkl')\n",
    "train.set_index('TransactionID', drop=True, inplace=True)\n",
    "\n",
    "test = pd.read_pickle('dataset/test_mem.pkl')\n",
    "test.set_index('TransactionID', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_sync (train_data, test_data, columns):\n",
    "#     '''\n",
    "#     Syncronizes test data with train data. Returns syncronized test data.\n",
    "#     '''\n",
    "#     for col in columns:\n",
    "#         unique_list = train_data[col].unique().tolist()\n",
    "#         test_data[col].loc[~test_data[col].isin(unique_list)] = np.nan\n",
    "#     return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_encoder(train_data, full_data, columns):\n",
    "#     for col in columns:\n",
    "#         full_data[col].fillna('missing', inplace=True)\n",
    "#         col_dict = train_data.groupby(col).isFraud.mean().to_dict()\n",
    "#         full_data[col+'_val'] = full_data[col].map(col_dict)\n",
    "#     return full_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test):\n",
    "    \n",
    "    ## PREPROCESSING\n",
    "    sync_list = (['P_emaildomain', 'R_emaildomain']\n",
    "                 +['DeviceInfo']\n",
    "                 +['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n",
    "                )\n",
    "    \n",
    "    # Preprocessing for unique values in the test set (sync_list columns)\n",
    "    for col in sync_list:\n",
    "        unique_list = train[col].unique().tolist()\n",
    "        test[col].loc[~test[col].isin(unique_list)] = np.nan\n",
    "    \n",
    "    # Concatenate train and test data\n",
    "    data = pd.concat((train.drop('isFraud', axis=1), test)).copy()\n",
    "\n",
    "    ## FEATURE SELECTION\n",
    "    # List of columns/features to be removed from the adtaaset\n",
    "    remove_list = (#['TransactionAmt'] # new (log transformed) variable created\n",
    "                   ['C2', 'C4', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14'] # correlation\n",
    "                  +['D2', 'D6', 'D7', 'D12'] # correlation\n",
    "                  +['V5', 'V9', 'V11', 'V13', 'V16', 'V17', 'V18', 'V20', 'V21', 'V22', 'V24', \n",
    "                    'V26', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V36', 'V38', 'V39', \n",
    "                    'V40', 'V41', 'V42', 'V43', 'V45', 'V48', 'V49', 'V50', 'V51', 'V52', 'V54', \n",
    "                    'V57', 'V58', 'V59', 'V60', 'V62', 'V63', 'V64', 'V65', 'V67', 'V68', 'V69', \n",
    "                    'V70', 'V71', 'V72', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V84', \n",
    "                    'V85', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V96', 'V97', \n",
    "                    'V100', 'V101', 'V102', 'V103', 'V105', 'V106', 'V110', 'V113', 'V116', 'V119', \n",
    "                    'V125', 'V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V137', 'V140', 'V142', \n",
    "                    'V143', 'V145', 'V147', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', \n",
    "                    'V156', 'V157', 'V158', 'V159', 'V160', 'V162', 'V163', 'V164', 'V165', 'V167', \n",
    "                    'V168', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V185', 'V186', \n",
    "                    'V189', 'V190', 'V191', 'V192', 'V193', 'V195', 'V196', 'V197', 'V198', 'V199', \n",
    "                    'V200', 'V201', 'V202', 'V203', 'V204', 'V206', 'V207', 'V211', 'V212', 'V213', \n",
    "                    'V216', 'V217', 'V218', 'V219', 'V222', 'V225', 'V228', 'V229', 'V230', 'V231', \n",
    "                    'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V239', 'V242', 'V243', 'V244', \n",
    "                    'V245', 'V246', 'V247', 'V248', 'V249', 'V251', 'V252', 'V253', 'V254', 'V255', \n",
    "                    'V256', 'V257', 'V258', 'V259', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', \n",
    "                    'V267', 'V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', \n",
    "                    'V278', 'V279', 'V280', 'V285', 'V287', 'V289', 'V292', 'V293', 'V294', 'V295', \n",
    "                    'V296', 'V297', 'V298', 'V299', 'V301', 'V302', 'V303', 'V304', 'V306', 'V307', \n",
    "                    'V308', 'V309', 'V310', 'V311', 'V312', 'V315', 'V316', 'V317', 'V318', 'V319', \n",
    "                    'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', \n",
    "                    'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339'] # correlation\n",
    "                  ) \n",
    "    # Remove unnecessary columns\n",
    "    data.drop(remove_list, axis=1, inplace=True)\n",
    "    \n",
    "    ## TRANSACTION \n",
    "    # Create a new column with the log of TransactionAmt\n",
    "    data['TransactionAmt_log'] = np.log(data.TransactionAmt)\n",
    "    \n",
    "    # New feature - decimal part of the transaction amount\n",
    "    data['TransactionAmt_decimal'] = ((data['TransactionAmt'] - data['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "    # Datetime\n",
    "    import datetime\n",
    "    START_DATE = '2019-03-01'\n",
    "    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    data['TransactionDT_DT'] = data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "    \n",
    "    # Hour column\n",
    "    hours = data['TransactionDT']/3600\n",
    "    data['Transaction_hours'] = np.floor(hours) % 24\n",
    "    \n",
    "    ## DEVICE\n",
    "    # Fill NaNs with \"missing\"\n",
    "    data['DeviceType'].fillna('missing', inplace=True)\n",
    "\n",
    "    ## M Columns\n",
    "    M_dict = {'T':1, 'F':0, 'M0':0, 'M1':1, 'M2':2}\n",
    "    data[['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']] = data[['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']].replace(M_dict)\n",
    "    \n",
    "    ## MEAN ENCODER\n",
    "    # Create new numeric column\n",
    "    for col in sync_list:\n",
    "        data[col].fillna('missing', inplace=True)\n",
    "        col_dict = train.groupby(col).isFraud.mean().to_dict()\n",
    "        data[col+'_val'] = data[col].map(col_dict)\n",
    "    \n",
    "    lbl_enc_list = (['P_emaildomain', 'R_emaildomain']\n",
    "             +['DeviceInfo']\n",
    "            )\n",
    "    \n",
    "    \n",
    "    ## New features from Kernels\n",
    "    data['TransactionAmt_to_std_card1'] = data['TransactionAmt'] / data.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "    data['TransactionAmt_to_mean_card1'] = data['TransactionAmt'] / data.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "    data['TransactionAmt_to_std_card4'] = data['TransactionAmt'] / data.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "    data['TransactionAmt_to_mean_card4'] = data['TransactionAmt'] / data.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "    \n",
    "    data['D15_to_mean_card1'] = data['D15'] / data.groupby(['card1'])['D15'].transform('mean')\n",
    "    data['D15_to_std_card1'] = data['D15'] / data.groupby(['card1'])['D15'].transform('std')\n",
    "    data['D15_to_mean_addr1'] = data['D15'] / data.groupby(['addr1'])['D15'].transform('mean')\n",
    "    data['D15_to_std_addr1'] = data['D15'] / data.groupby(['addr1'])['D15'].transform('std')\n",
    "    \n",
    "    # Replace inf values with nan\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)   \n",
    "    \n",
    "    ## DROP UNNECESSARY COLUMNS\n",
    "    # List of columns to be dropped from the dataset for modeling\n",
    "    drop_list = (['TransactionDT'] # new (same) variable created\n",
    "                +['TransactionDT_DT'] # datetime \n",
    "                + sync_list # new (encoded) variables created\n",
    "                )\n",
    "    \n",
    "    # Create a list of columns with more than 20% missing \n",
    "    # remove_missing_cols = data.isna().mean()[data.isna().mean()>0.2].index.to_list()\n",
    "    \n",
    "    # Drop unnecessary columns \n",
    "    # data.drop(drop_list+remove_missing_cols, axis=1, inplace=True)\n",
    "    data.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    ## LABEL ENCODER\n",
    "#     for col in data.columns:\n",
    "#         if data[col].dtype=='object': \n",
    "#             lbl = preprocessing.LabelEncoder()\n",
    "#             data[col] = lbl.fit_transform(data[col].values)\n",
    "    \n",
    "    ## PREPARE DATA FOR MODELING\n",
    "    # Create dummy variables\n",
    "    data = pd.get_dummies(data)\n",
    "    \n",
    "    # Split train and test data\n",
    "    train_data = data[:train.shape[0]]\n",
    "    test_data = data[train.shape[0]:]\n",
    "    \n",
    "    return train_data.join(train.isFraud), test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def feature_engineering(train, test):\n",
    "    \n",
    "#     ## PREPROCESSING\n",
    "#     sync_list = (['P_emaildomain', 'R_emaildomain']\n",
    "#                  +['DeviceInfo']\n",
    "#                 )\n",
    "    \n",
    "#     # Preprocessing for unique values in the test set (e-mail columns)\n",
    "#     P_email_list = train.P_emaildomain.unique().tolist()\n",
    "#     test.P_emaildomain.loc[~test.P_emaildomain.isin(P_email_list)] = np.nan\n",
    "    \n",
    "#     R_email_list = train.R_emaildomain.unique().tolist()\n",
    "#     test.R_emaildomain.loc[~test.R_emaildomain.isin(R_email_list)] = np.nan\n",
    "    \n",
    "#     # Preprocessing for unique values in the test set (DeviceInfo columns)\n",
    "#     device_list = train.DeviceInfo.unique().tolist()\n",
    "#     test.DeviceInfo.loc[~test.DeviceInfo.isin(device_list)] = np.nan\n",
    "    \n",
    "    \n",
    "#     # Concatenate train and test data\n",
    "#     data = pd.concat((train.drop('isFraud', axis=1), test)).copy()\n",
    "\n",
    "#     ## FEATURE SELECTION\n",
    "#     # List of columns/features to be removed from the adtaaset\n",
    "#     remove_list = (#['TransactionAmt'] # new (log transformed) variable created\n",
    "#                    ['C2', 'C4', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14'] # correlation\n",
    "#                   +['D2', 'D6', 'D7', 'D12'] # correlation\n",
    "#                   +['V5', 'V9', 'V11', 'V13', 'V16', 'V17', 'V18', 'V20', 'V21', 'V22', 'V24', \n",
    "#                     'V26', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V36', 'V38', 'V39', \n",
    "#                     'V40', 'V41', 'V42', 'V43', 'V45', 'V48', 'V49', 'V50', 'V51', 'V52', 'V54', \n",
    "#                     'V57', 'V58', 'V59', 'V60', 'V62', 'V63', 'V64', 'V65', 'V67', 'V68', 'V69', \n",
    "#                     'V70', 'V71', 'V72', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V84', \n",
    "#                     'V85', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V96', 'V97', \n",
    "#                     'V100', 'V101', 'V102', 'V103', 'V105', 'V106', 'V110', 'V113', 'V116', 'V119', \n",
    "#                     'V125', 'V126', 'V127', 'V128', 'V132', 'V133', 'V134', 'V137', 'V140', 'V142', \n",
    "#                     'V143', 'V145', 'V147', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', \n",
    "#                     'V156', 'V157', 'V158', 'V159', 'V160', 'V162', 'V163', 'V164', 'V165', 'V167', \n",
    "#                     'V168', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V185', 'V186', \n",
    "#                     'V189', 'V190', 'V191', 'V192', 'V193', 'V195', 'V196', 'V197', 'V198', 'V199', \n",
    "#                     'V200', 'V201', 'V202', 'V203', 'V204', 'V206', 'V207', 'V211', 'V212', 'V213', \n",
    "#                     'V216', 'V217', 'V218', 'V219', 'V222', 'V225', 'V228', 'V229', 'V230', 'V231', \n",
    "#                     'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V239', 'V242', 'V243', 'V244', \n",
    "#                     'V245', 'V246', 'V247', 'V248', 'V249', 'V251', 'V252', 'V253', 'V254', 'V255', \n",
    "#                     'V256', 'V257', 'V258', 'V259', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', \n",
    "#                     'V267', 'V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', \n",
    "#                     'V278', 'V279', 'V280', 'V285', 'V287', 'V289', 'V292', 'V293', 'V294', 'V295', \n",
    "#                     'V296', 'V297', 'V298', 'V299', 'V301', 'V302', 'V303', 'V304', 'V306', 'V307', \n",
    "#                     'V308', 'V309', 'V310', 'V311', 'V312', 'V315', 'V316', 'V317', 'V318', 'V319', \n",
    "#                     'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', \n",
    "#                     'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339'] # correlation\n",
    "#                   ## FOR NOW\n",
    "#                   +['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', \n",
    "#                      'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "#                      'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', \n",
    "#                      'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', \n",
    "#                      'id_37', 'id_38'] # Needs feature engineering and missing val analysis\n",
    "#                   ) \n",
    "#     # Remove unnecessary columns\n",
    "#     data.drop(remove_list, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "#     ## TRANSACTION \n",
    "#     # Create a new column with the log of TransactionAmt\n",
    "#     data['TransactionAmt_log'] = np.log(data.TransactionAmt)\n",
    "    \n",
    "#     # Datetime\n",
    "#     import datetime\n",
    "#     START_DATE = '2019-03-01'\n",
    "#     startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "#     data['TransactionDT_DT'] = data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "    \n",
    "#     # Hour column\n",
    "#     hours = data['TransactionDT']/3600\n",
    "#     data['Transaction_hours'] = np.floor(hours) % 24\n",
    "\n",
    "#     ## E-MAIL \n",
    "#     # Fill NaNs with \"missing\"\n",
    "#     data.P_emaildomain.fillna('missing', inplace=True)\n",
    "#     data.P_emaildomain.fillna('missing', inplace=True)\n",
    "\n",
    "#     # Create dict for each column (using train data)\n",
    "#     P_email_dict = train.groupby('P_emaildomain').isFraud.mean().to_dict()\n",
    "#     R_email_dict = train.groupby('R_emaildomain').isFraud.mean().to_dict()\n",
    "\n",
    "#     # Create new numeric column\n",
    "#     data['P_emaildomain_val'] = data.P_emaildomain.map(P_email_dict)\n",
    "#     data['R_emaildomain_val'] = data.R_emaildomain.map(R_email_dict)\n",
    "    \n",
    "#     ## DEVICE\n",
    "#     # Fill NaNs with \"missing\"\n",
    "#     data['DeviceType'].fillna('missing', inplace=True)\n",
    "\n",
    "#     # Create new numeric column\n",
    "#     data.DeviceInfo.fillna('missing', inplace=True)\n",
    "#     device_dict = train.groupby('DeviceInfo').isFraud.mean().to_dict()\n",
    "#     data['DeviceInfo_val'] = data.DeviceInfo.map(device_dict)\n",
    "    \n",
    "#     ## PREPARE DATA FOR MODEL\n",
    "#     # List of columns to be dropped from the dataset for modeling\n",
    "#     drop_list = (['TransactionDT'] # new (same) variable created\n",
    "#                 +['TransactionDT_DT'] # datetime \n",
    "#                 +['P_emaildomain', 'R_emaildomain'] # new (encoded) variable created\n",
    "#                 +['DeviceInfo'] # new (encoded) variable created\n",
    "#                 )\n",
    "    \n",
    "#     # Create a list of columns with more than 20% missing \n",
    "#     remove_missing_cols = data.isna().mean()[data.isna().mean()>0.5].index.to_list()\n",
    "    \n",
    "#     # Drop unnecessary columns \n",
    "#     data.drop(drop_list+remove_missing_cols, axis=1, inplace=True)\n",
    "    \n",
    "#     # Create dummy variables\n",
    "#     data = pd.get_dummies(data)\n",
    "    \n",
    "#     # Split train and test data\n",
    "#     train_data = data[:train.shape[0]]\n",
    "#     test_data = data[train.shape[0]:]\n",
    "    \n",
    "#     return train_data.join(train.isFraud), test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train, test = feature_engineering(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 204), (506691, 203))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C3', 'C5', 'D1', 'D3', 'D4', 'D5', 'D8', 'D9', 'D10', 'D11', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V6', 'V7', 'V8', 'V10', 'V12', 'V14', 'V15', 'V19', 'V23', 'V25', 'V27', 'V35', 'V37', 'V44', 'V46', 'V47', 'V53', 'V55', 'V56', 'V61', 'V66', 'V75', 'V77', 'V78', 'V82', 'V86', 'V95', 'V98', 'V99', 'V104', 'V107', 'V108', 'V109', 'V111', 'V112', 'V114', 'V115', 'V117', 'V118', 'V120', 'V121', 'V122', 'V123', 'V124', 'V129', 'V130', 'V131', 'V135', 'V136', 'V138', 'V139', 'V141', 'V144', 'V146', 'V148', 'V161', 'V166', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V184', 'V187', 'V188', 'V194', 'V205', 'V208', 'V209', 'V210', 'V214', 'V215', 'V220', 'V221', 'V223', 'V224', 'V226', 'V227', 'V238', 'V240', 'V241', 'V250', 'V260', 'V270', 'V281', 'V282', 'V283', 'V284', 'V286', 'V288', 'V290', 'V291', 'V300', 'V305', 'V313', 'V314', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'id_32', 'TransactionAmt_log', 'TransactionAmt_decimal', 'Transaction_hours', 'P_emaildomain_val', 'R_emaildomain_val', 'DeviceInfo_val', 'id_12_val', 'id_15_val', 'id_16_val', 'id_23_val', 'id_27_val', 'id_28_val', 'id_29_val', 'id_30_val', 'id_31_val', 'id_33_val', 'id_34_val', 'id_35_val', 'id_36_val', 'id_37_val', 'id_38_val', 'TransactionAmt_to_std_card1', 'TransactionAmt_to_mean_card1', 'TransactionAmt_to_std_card4', 'TransactionAmt_to_mean_card4', 'D15_to_mean_card1', 'D15_to_std_card1', 'D15_to_mean_addr1', 'D15_to_std_addr1', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R', 'ProductCD_S', 'ProductCD_W', 'card4_american express', 'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card', 'card6_credit', 'card6_debit', 'card6_debit or credit', 'DeviceType_desktop', 'DeviceType_missing', 'DeviceType_mobile', 'isFraud']\n"
     ]
    }
   ],
   "source": [
    "print(train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train, 'dataset/train_engineered_new2.pkl')\n",
    "pd.to_pickle(test, 'dataset/test_engineered_new2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the log of TransactionAmt\n",
    "train['TransactionAmt_log'] = np.log(train.TransactionAmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "START_DATE = '2019-03-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "train['TransactionDT_DT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hour_feature(df, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    tname : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    hours = df[tname] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours\n",
    "\n",
    "train['Transaction_hours'] = make_hour_feature(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with \"missing\"\n",
    "train.P_emaildomain.fillna('missing', inplace=True)\n",
    "train.R_emaildomain = train.P_emaildomain.fillna('missing')\n",
    "\n",
    "# Create dict for each columns (-> use these dicts in for converting TEST DATA)\n",
    "P_email_dict = train.groupby('P_emaildomain').isFraud.mean().to_dict()\n",
    "R_email_dict = train.groupby('R_emaildomain').isFraud.mean().to_dict()\n",
    "\n",
    "# Create new numeric columns\n",
    "train['P_emaildomain_val'] = train.P_emaildomain.map(P_email_dict)\n",
    "train['R_emaildomain_val'] = train.R_emaildomain.map(R_email_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-96bf6b4da8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Apply the encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             Xi = check_array(Xi, ensure_2d=False, dtype=None,\n\u001b[0;32m---> 67\u001b[0;31m                              force_all_finite=needs_validation)\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mX_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create the encoder.\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "encoder.fit(train)\n",
    "\n",
    "# Apply the encoder.\n",
    "train_encoded = encoder.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 439)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_list + drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train = train.drop(remove_list + drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 144)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(save_train, 'dataset/train_engineered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
